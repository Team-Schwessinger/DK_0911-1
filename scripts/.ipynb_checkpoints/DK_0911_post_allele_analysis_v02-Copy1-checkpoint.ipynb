{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pst_104E_v12_post_allele_analysis_v02_RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on original code by Benjamin Schwessinger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: output from `Pst_104E_defining_alleles_v02` & primary+haplotig (ph) protein/gene/cds .*fasta* files.\n",
    "- Programs: **MUSCLE**, **PAML**\n",
    "- Purpose: generate and save a DataFrame containing dN/dS information (number of nonsynonymous substitutions per non-synonymous site to the number of synonymous substitutions per synonymous site), as well as Hamming & Levenshtein distances (measures of % identity). Also provides visualisations of some of this data.\n",
    "\n",
    "#### Overview\n",
    "1. Reads in the large allele DataFrames generated in `Pst_104E_defining_alleles_v02` (i.e. proteinortho hits OR best blast hit) - see description header cell in that notebook for more information on which alleles are included in that DataFrame.\n",
    "2. Filters the allele DataFrames based on %ID and %QCov (this can be set to filter only BLAST-identified alleles or both BLAST- and proteinortho-identified alleles) so that distance information is not calculated on an unnecessarily large number of alleles.\n",
    "3. Calculates distance & dN/dS information, and saves this to an output file so that it does not have to be re-calculated (if for whatever reason, the inputs change so that dN/dS or distance information should change, this output file (`xx_analysed_alleles.df`) should be deleted so that it can be re-generated.\n",
    "4. Plots graphs of allele-type distribution (pie chart) and allele-type Levenshtein distances (measures of similarity) for different levels of allele-filtering (QCov/TCov/%ID/Levenshtein similarity).\n",
    "\n",
    "NB:\n",
    "- dN/dS information is currently not utilised in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:12:45.335616Z",
     "start_time": "2018-03-06T00:12:44.894445Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:12:47.450000Z",
     "start_time": "2018-03-06T00:12:45.829447Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import distance\n",
    "import editdistance\n",
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import collections\n",
    "import pybedtools\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:22:37.398066Z",
     "start_time": "2018-03-06T00:22:37.314753Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GENOME_VERSION = 'Pst_104E_v12'\n",
    "\n",
    "BASE_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/Warrior_comp_runs/allele_analysis'\n",
    "\n",
    "YN00_PATH = os.path.join(BASE_PATH, 'post_allele_analysis', 'yn00.ctl')\n",
    "BASE_OUT_PATH = os.path.join(BASE_PATH, 'post_allele_analysis')\n",
    "ALLELE_PATH = os.path.join(BASE_PATH,'defining_alleles/allele_analysis/alleles_proteinortho_graph516' )\n",
    "UNFILTERED_DF_PATH = os.path.join(BASE_PATH,'defining_alleles/allele_analysis',\\\n",
    "                '%s_p_ctg.%s_h_ctg.0.001.blastp.outfmt6.allele_analysis' % (GENOME_VERSION, GENOME_VERSION))\n",
    "GENOME_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'\n",
    "FIGURE_PATH = os.path.join(BASE_OUT_PATH, 'figures')\n",
    "\n",
    "GENOME = GENOME_VERSION\n",
    "P_GENOME = GENOME + '_p_ctg'\n",
    "H_GENOME = GENOME + '_h_ctg'\n",
    "threads = 8\n",
    "\n",
    "# Base filtering so that distance calculations are not performed on all allele pairs.\n",
    "# Distance calculations will only be performed on allele pairs above the defined cutoffs.\n",
    "# Note that proteinortho alleles will not be affected (this can be changed in the filterAlleleDf function).\n",
    "BASE_QCOV_CUTOFF = 0\n",
    "BASE_TCOV_CUTOFF = 0\n",
    "BASE_PCTID_CUTOFF = 0\n",
    "\n",
    "P_PROTEINS_FASTA = os.path.join(GENOME_PATH, P_GENOME + '.anno.protein.fa')\n",
    "PH_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.protein.fa')\n",
    "PH_GENE_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.gene.fa')\n",
    "PH_CDS_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.CDS.fa')\n",
    "\n",
    "PAML_PATH = os.path.join(BASE_OUT_PATH, 'paml')\n",
    "if not os.path.exists(BASE_OUT_PATH):\n",
    "    os.mkdir(BASE_OUT_PATH)\n",
    "if not os.path.exists(FIGURE_PATH):\n",
    "    os.mkdir(FIGURE_PATH)\n",
    "if not os.path.exists(PAML_PATH):\n",
    "    os.mkdir(PAML_PATH)\n",
    "    shutil.copy2(YN00_PATH, PAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:53.791549Z",
     "start_time": "2018-03-06T00:17:53.786281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for homozygous region analysis\n",
    "COV_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/COV'\n",
    "homo_bed_fh = os.path.join(COV_PATH, 'Pst_104E_v12_ph_ctg.ph_p_homo_cov.bed')\n",
    "anno_gff_p_fh = os.path.join(GENOME_PATH, 'Pst_104E_v12_p_ctg.sorted.anno.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:20:21.166213Z",
     "start_time": "2018-03-06T00:20:21.151971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PH_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.protein.fa')\n",
    "P_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_p_ctg.anno.protein.fa')\n",
    "H_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_h_ctg.anno.protein.fa')\n",
    "PH_GENE_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.gene.fa')\n",
    "PH_CDS_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.CDS.fa')\n",
    "PH_GFF_FH = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:04.781181Z",
     "start_time": "2018-03-06T00:17:04.775121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is only for Pst_104E_post_analysis starting with version v12 required as only v13 has th locus_tags attached\n",
    "PH_GFF_DICT_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/gitrepo_sub_21092017/Assembly'\n",
    "PH_GFF_DICT_FH = os.path.join(PH_GFF_DICT_PATH, 'Pst_104E_v13_ph_ctg.anno.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:29.171739Z",
     "start_time": "2018-03-06T00:17:29.156321Z"
    }
   },
   "outputs": [],
   "source": [
    "#for finalizing the protein ortho single analysis\n",
    "proortho_fh = os.path.join(BASE_PATH, 'defining_alleles','proteinortho', 'ph_ctg_516.proteinortho')\n",
    "proorthograph_fh = os.path.join(BASE_PATH,'defining_alleles', 'proteinortho', 'ph_ctg_516.proteinortho-graph')\n",
    "poff_graph_fh = os.path.join(BASE_PATH, 'defining_alleles','proteinortho', 'ph_ctg_516.poff-graph')\n",
    "assert(os.path.exists(proortho_fh))\n",
    "assert(os.path.exists(proorthograph_fh))\n",
    "assert(os.path.exists(poff_graph_fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:31.151044Z",
     "start_time": "2018-03-06T00:17:31.123154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInterHaplotypeParaloges(proorthograph_fh,poff_graph_fh):\n",
    "    \"\"\"This function compares the proteinorth (po) and po synteny output. \n",
    "    It returns all Target+Query pairs that are only found in po as list. \n",
    "    It also checks if there are pairs only found in poff.\"\"\"\n",
    "    graph_header = ['Target', 'Query', 'evalue_ab', 'bitscore_ab', 'evalue_ba', 'bitscore_ba', 'same_strand' , 'simscore']\n",
    "    poff_df = pd.read_csv(poff_graph_fh, sep='\\t', header=None, names=graph_header, comment='#' )\n",
    "    po_df = pd.read_csv(proorthograph_fh, sep='\\t', header=None, names=graph_header, comment='#' )\n",
    "    poff_paring = (poff_df.Target+ '+' + poff_df.Query)\n",
    "    po_paring = (po_df.Target+ '+' + po_df.Query)\n",
    "    interhaplotypeparaloges_l = list(set(po_paring) - set(poff_paring))\n",
    "    poff_only_pairs = list(set(poff_paring) - set(po_paring))\n",
    "    if len(poff_only_pairs) > 0:\n",
    "        print(\"Check poff and proteinortho output as conceptually all poff pairs should be in included in the proteinortho output.\\nThese are the pairs:\")\n",
    "        print(poff_only_pairs)\n",
    "    return interhaplotypeparaloges_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:31.532370Z",
     "start_time": "2018-03-06T00:17:31.517893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_INP(aTargetplusaQuery, proorthograph_fh = proorthograph_fh,poff_graph_fh = poff_graph_fh ):\n",
    "    \"\"\"\n",
    "    Returns a series of of True/False values if a aTarget+aQuery pair are element identified as \n",
    "    Inter haplotype paraloge.\n",
    "    Input:\n",
    "        proorthograh_fh, proteinortho graph output file handle that has been run including the single flag\n",
    "        queries, series of protein queries used for the blast search.\n",
    "        poff_graph_fh, proteinortho graph output file handle that has been run including the synteny and\n",
    "        single flag\n",
    "        aTargetplusaQuery, series of protein aTargets + aQuiers used for the blast search.\n",
    "    Outupt:\n",
    "        Returns a True/False series with the aTargets + aQuiers as index.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    interhaplotypeparaloges_l = getInterHaplotypeParaloges(proorthograph_fh,poff_graph_fh)\n",
    "    interhaplotypeparaloges_s = pd.Series([False]*len(aTargetplusaQuery), index=aTargetplusaQuery)\n",
    "    interhaplotypeparaloges_s.loc[interhaplotypeparaloges_s.index.isin(interhaplotypeparaloges_l)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return interhaplotypeparaloges_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:31.951753Z",
     "start_time": "2018-03-06T00:17:31.938643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProOrthsingles(proortho_fh):\n",
    "    \"\"\"Parses out all proteinortho singles from a proteinortho output file.\n",
    "    Returns a list of these singles.\"\"\"\n",
    "    with open(proortho_fh) as file:\n",
    "        id_pattern = r'(evm\\S*)'\n",
    "        regex = re.compile(id_pattern)\n",
    "        po_single_list = []\n",
    "        for line in file:\n",
    "            if '*' in line:\n",
    "                po_single_list.append(regex.search(line).groups()[0])\n",
    "            else:\n",
    "                continue\n",
    "    return po_single_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:32.445412Z",
     "start_time": "2018-03-06T00:17:32.435840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_PO_single(queries, proortho_fh=proortho_fh):\n",
    "    \"\"\"\n",
    "    Returns a series of of True/False values if a query element identified as single.\n",
    "    Input:\n",
    "        proortho_fh, proteinortho output file handle that has been run including the single flag\n",
    "        queries, series of protein queries used for the blast search.\n",
    "    Outupt:\n",
    "        Returns a True/False series with the queries as index.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    PO_singles = getProOrthsingles(proortho_fh)\n",
    "    PO_singles_s = pd.Series([False]*len(queries), index=queries)\n",
    "    PO_singles_s.loc[PO_singles_s.index.isin(PO_singles)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return PO_singles_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:32.693905Z",
     "start_time": "2018-03-06T00:17:32.684517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allgffTogenegff(gff_fh, write_out=True):\n",
    "    '''Converts at complete gff to a gene gff only and writtes it out.'''\n",
    "    gene_gff = pd.read_csv(gff_fh, sep='\\t', header=None)\n",
    "    gene_gff = gene_gff[gene_gff[2] == 'gene']\n",
    "    gene_gff.reset_index(drop=True, inplace=True)\n",
    "    gene_gff.to_csv(gff_fh.replace('anno', 'gene'), sep='\\t', header=False, index=False)\n",
    "    return gene_gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:32.912132Z",
     "start_time": "2018-03-06T00:17:32.890886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_8_id(x):\n",
    "    '''Function that pulls out the ID from the 9th column of a df.'''\n",
    "    pattern = r'ID=([a-zA-Z0-9_.]*);'\n",
    "    regex = re.compile(pattern)  \n",
    "    m = regex.search(x)\n",
    "    match = m.groups()[0].replace('TU', 'model')\n",
    "    if match.startswith('cds.'):\n",
    "        match = match[4:]\n",
    "    if 'exon' in match:\n",
    "        _list = match.split('.')\n",
    "        match = '.'.join(_list[:-1])\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:33.190975Z",
     "start_time": "2018-03-06T00:17:33.073647Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignMatchType(allele_source, overlap, no_overlap):\n",
    "    if pd.isnull(allele_source):\n",
    "        return allele_source\n",
    "    \n",
    "    s = allele_source + '_'\n",
    "    \n",
    "    if overlap ==  True:\n",
    "        s += 'overlap'\n",
    "    elif no_overlap == True:\n",
    "        s += 'no_overlap'\n",
    "    else: # different_pcontig\n",
    "        s += 'unlinked'\n",
    "    return s\n",
    "\n",
    "def reduceGroups(g):\n",
    "    '''returns the best hit based on e-value and BitScore per group'''\n",
    "    if len(g) == 1:\n",
    "        return g\n",
    "    tmp_g = g[g['e-value'] == g['e-value'].min()]\n",
    "    if len(tmp_g) == 1:\n",
    "        return tmp_g\n",
    "    return tmp_g[tmp_g['BitScore'] == tmp_g['BitScore'].max()]\n",
    "\n",
    "def filterAlleleDf(alleleDf, qCov, tCov, pctId, levSim, leavePO=False):\n",
    "    if leavePO:\n",
    "        no_PO_df = alleleDf[(alleleDf['allele_source'] == 'h_rBLAST') | (alleleDf['allele_source'] == 'BLAST')]\n",
    "        PO_df = alleleDf[alleleDf['allele_source'] == 'PO']\n",
    "\n",
    "        filtered_no_PO_df = filterAlleleDf(no_PO_df, qCov, tCov, pctId, levSim)\n",
    "        return filtered_no_PO_df.append(PO_df, ignore_index=True)\n",
    "    \n",
    "    if qCov:\n",
    "        alleleDf = alleleDf[alleleDf['QCov'] > qCov]\n",
    "    if tCov:\n",
    "        alleleDf = alleleDf[alleleDf['TCov'] > tCov]\n",
    "    if pctId:\n",
    "        alleleDf = alleleDf[alleleDf['PctID'] > pctId]\n",
    "    if levSim:\n",
    "        levDist = (100-levSim)/100.0\n",
    "        alleleDf = alleleDf[alleleDf['protein_levenshtein'] < levDist]\n",
    "\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:33.270592Z",
     "start_time": "2018-03-06T00:17:33.257160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geneUnphased(Gene_gff_fh, Homo_cov_bed_fh ):\n",
    "    \"\"\"\n",
    "    Returns a list of all genes that are unphased.\n",
    "    \n",
    "    Input: * Fh for annotation gff file\n",
    "           * Fh for Homo_cov_bed_fh\n",
    "    Output: A set of gene IDs that are unphases\n",
    "    \"\"\"\n",
    "    geneGff_bed = pybedtools.BedTool(Gene_gff_fh)\n",
    "    homo_p_bed = pybedtools.BedTool(Homo_cov_bed_fh)\n",
    "    gene_ids_ph_p_homo = []\n",
    "    for x in geneGff_bed.intersect(homo_p_bed, f=0.4):\n",
    "        y = col_8_id(x[8])\n",
    "        gene_ids_ph_p_homo.append(y)\n",
    "    gene_ids_ph_p_homo = set(gene_ids_ph_p_homo)\n",
    "    return gene_ids_ph_p_homo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:17:59.939833Z",
     "start_time": "2018-03-06T00:17:59.919291Z"
    }
   },
   "outputs": [],
   "source": [
    "def assign_unphased(queries, gff_fh=anno_gff_p_fh, homo_bed_fh=homo_bed_fh):\n",
    "    \"\"\"\n",
    "    Takes a series of gene models and checks if they overlap with the\n",
    "    homo_bed dataframe. The minimum overlap is f=0.4 with the intersect function.\n",
    "    \n",
    "    TO DO: For now the homo_bed only includes primary gene models. Should be changed.\n",
    "    Input:  \n",
    "        queries, pd.Series of gene names. \n",
    "        gff_fh, file handle of gff file.\n",
    "        homo_bed_dfh, file handle of bed file that represents non-phased regions.\n",
    "    \n",
    "    \"\"\"\n",
    "    if '.gene' not in gff_fh: \n",
    "        _ = allgffTogenegff(gff_fh)\n",
    "    Unphasedgenes = geneUnphased(gff_fh.replace('anno', 'gene'), homo_bed_fh)\n",
    "    unphased_s = pd.Series([False]*len(queries), index=queries)\n",
    "    unphased_s.loc[unphased_s.index.isin(Unphasedgenes)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return unphased_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:00.522293Z",
     "start_time": "2018-03-06T00:18:00.513206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFastaDict(fastaFile):\n",
    "    d = {}\n",
    "    for gene in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        d[gene.id] = gene\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:01.174936Z",
     "start_time": "2018-03-06T00:18:01.106134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeAllelicFasta(alleleOne, alleleTwo, alleleType, outPath):\n",
    "    '''writes fasta file containing fasta information for two alleles\n",
    "    in the outPath'''\n",
    "    assert(alleleType.upper() in ['CDS', 'GENE', 'PROTEIN'])\n",
    "    \n",
    "    seqRecordDict = globals()['SEQRECORD_' + alleleType.upper() + '_DICT']\n",
    "    try:\n",
    "        alleleSeqRecords = [seqRecordDict[alleleOne], seqRecordDict[alleleTwo]]\n",
    "    except KeyError:\n",
    "        print(alleleOne)\n",
    "        print(alleleTwo)\n",
    "        print(alleleType)\n",
    "        sys.exit()\n",
    "    with open(os.path.join(outPath, alleleType.lower() + '.fa'), 'w') as outFile:\n",
    "        SeqIO.write(alleleSeqRecords, outFile, 'fasta')\n",
    "    return True\n",
    "\n",
    "def writeAlignmentScript(alleleOutPath, scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'a') as outFile:\n",
    "        print('cd %s' % alleleOutPath, file=outFile)\n",
    "        print('/home/gamran/anaconda3/muscle3.8.31_i86linux64 -clwstrict -in protein.fa -out protein.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl -output paml protein.aln cds.fa > cds_codon.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl protein.aln cds.fa > cds_codon.clustal', file=outFile)\n",
    "        print('cp %s/yn00.ctl ./' % PAML_PATH, file=outFile)\n",
    "        print('/home/gamran/anaconda3/paml4.9g/bin/yn00', file=outFile)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:01.668626Z",
     "start_time": "2018-03-06T00:18:01.626091Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareAlignmentBashScript(scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'w') as pamlScript:\n",
    "        print('#!/bin/bash', file=pamlScript)\n",
    "\n",
    "    for index, [Query, Target] in alleleDf.iloc[:, :2].iterrows():\n",
    "        #if we don't have a blast hit skip.\n",
    "        if pd.isnull(Target):\n",
    "            continue\n",
    "        else:\n",
    "            alleleOutPath = os.path.join(PAML_PATH, '%s_%s' % (Query, Target))\n",
    "            if not os.path.exists(alleleOutPath):\n",
    "                os.mkdir(os.path.join(PAML_PATH, '%s_%s' % (Query, Target)))\n",
    "\n",
    "            writeAllelicFasta(Query, Target, 'CDS', alleleOutPath)\n",
    "            writeAllelicFasta(Query, Target, 'PROTEIN', alleleOutPath)\n",
    "\n",
    "            writeAlignmentScript(alleleOutPath, os.path.join(PAML_PATH, 'paml_script.sh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:02.484229Z",
     "start_time": "2018-03-06T00:18:02.322279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignDistancesToAlleles(folder, alignmentFile, alleleType):\n",
    "    '''Adds Hamming and Levenshtein distance columns to an allele pair\n",
    "    (indexed by 'folder' name) in df'''\n",
    "    #print(folder)\n",
    "    if pd.isnull(folder):\n",
    "        return np.nan, np.nan\n",
    "    assert(alleleType.upper() in ['PROTEIN', 'CDS', 'GENE'])\n",
    "    seq1, seq2 = AlignIO.read(open(alignmentFile, 'r'), format='clustal', seq_count=2)\n",
    "    seq1 = str(seq1.seq).upper()\n",
    "    seq2 = str(seq2.seq).upper()\n",
    "    assert(len(seq1) == len(seq2))\n",
    "    return editdistance.eval(seq1, seq2)/len(seq1), distance.hamming(seq1, seq2, normalized=True)\n",
    "\n",
    "def assignDistancesToAllAlleles(df_folder_index, all_folders, tmp_path, suffix):\n",
    "    \"\"\"\n",
    "    Reads in the index that contains the folder pairings for the alignements.\n",
    "    Returns a protein_df and CDS_df that contain the hamming and levenshtein distance each.\n",
    "    \"\"\"\n",
    "    cleaned_index = [x for x in df_folder_index if x in all_folders]\n",
    "    count = 0\n",
    "    total = len(df_folder_index)\n",
    "    percentDone = 0\n",
    "    protein_lev_dict = {}\n",
    "    protein_ham_dict = {}\n",
    "    CDS_lev_dict = {}\n",
    "    CDS_ham_dict = {}\n",
    "    \n",
    "    #print(\"Calculating distances and adding them to the allele DataFrame...\")\n",
    "    \n",
    "    for folder in cleaned_index:\n",
    "        if pd.isnull(folder):\n",
    "            proteinAlignmentFile = ''\n",
    "            cdsAlignmentFile = ''\n",
    "        else:\n",
    "            proteinAlignmentFile = os.path.join(PAML_PATH, folder, 'protein.aln')\n",
    "            cdsAlignmentFile = os.path.join(PAML_PATH, folder, 'cds_codon.clustal')\n",
    "        #here the nan get overwritten. This doesn't matter though as they are all\n",
    "        #nan anyway.\n",
    "        protein_lev_dict[folder], protein_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, proteinAlignmentFile, 'PROTEIN')\n",
    "        CDS_lev_dict[folder], CDS_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, cdsAlignmentFile, 'CDS')\n",
    "\n",
    "        count += 1\n",
    "        #if round(count/total * 100) > percentDone:\n",
    "            #percentDone = round(count/total * 100)\n",
    "            #print(\"%s%% complete\" % percentDone)\n",
    "            \n",
    "    newdf_columns=['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    if len(protein_ham_dict) > 0:\n",
    "        df = pd.DataFrame([protein_ham_dict,protein_lev_dict,CDS_ham_dict,CDS_lev_dict]).T\n",
    "        df.rename(columns=dict(zip(df.columns,newdf_columns)),inplace=True)\n",
    "        out_name = os.path.join(tmp_path, '%s_%s.%s' % (df.index[0],df.index[-1],suffix))\n",
    "        df.round(4).to_csv(out_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:03.220893Z",
     "start_time": "2018-03-06T00:18:02.986081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dNdS_to_df(line, folder):\n",
    "    \"\"\"\n",
    "    Function that parses out dN and dS of a yn00 file and calls the \n",
    "    assign_dNdS function. Therefore returns a single element\n",
    "    pd.Series with folder name as index.\n",
    "    \"\"\"\n",
    "    dN = re.findall(r'dN = [-| ]?(.*) w', line)[0]\n",
    "    dS = re.findall(r'dS = [-| ]?(.*) dN', line)[0]\n",
    "    return assign_dNdS(dN, dS, folder)\n",
    "\n",
    "def assign_dNdS(dN, dS, folder):\n",
    "    '''\n",
    "    Function that cacluates the dN/dS ratio an returns it as a series usind the folder as index.\n",
    "    Input: dN, dS, folder(name)\n",
    "    Output: single element pd.Series with folder name as index.\n",
    "    '''\n",
    "    if float(dS) > 0:\n",
    "        series = pd.Series([float(dN)/float(dS)], index=[folder])\n",
    "    else:\n",
    "        series = pd.Series([np.nan], index=[folder])\n",
    "    return series\n",
    "\n",
    "def assign_dNdS_to_all_alleles(folder_index, all_folders, tmp_path, suffix):\n",
    "    \"\"\"Function that parses out the different dN/dS ratios from a yn.out file for a list/index\n",
    "    of folders that contain these yn.out files. The output dataframe is saved to tmp folder using\n",
    "    the suffix.\n",
    "    Input:  folder_index, list or index where to find the yn.out for each pairing.\n",
    "            all_folders, are all possible folders. This is used to filter out nan and so from\n",
    "                paralizing etc.\n",
    "            tmp_path, is the path were \n",
    "    Output: Saved out tmp df with the suffix as file ending.\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_index = [x for x in folder_index if x in all_folders]\n",
    "    #print(cleaned_index)\n",
    "    yn00_s = pd.Series([], name='yn00_dN/dS')\n",
    "    LWL85_s = pd.Series([], name='LWL85_dN/dS')\n",
    "    LWL85m_s = pd.Series([], name='LWL85m_dN/dS')\n",
    "    LPB93_s = pd.Series([], name='LPB93_dN/dS')\n",
    "    #header = ['folder','yn00_dN/dS', 'LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS']\n",
    "    #append these list\n",
    "    for folder in cleaned_index:\n",
    "        alleleYn = os.path.join(PAML_PATH, folder,'yn.out')\n",
    "        with open(alleleYn, 'r') as ynOut:\n",
    "            #now loop over the lines and parse out stuff\n",
    "            for i, line in enumerate(ynOut):\n",
    "                if line.startswith('seq. seq. ') and i > 0:\n",
    "                    next(ynOut) # we want the line that is two after the line starting with 'seq. seq '\n",
    "                    dataLine = next(ynOut)\n",
    "                    dN = dataLine.split('+-')[0].rstrip().split(' ')[-1]\n",
    "                    dS = dataLine.split('+-')[1].rstrip().split(' ')[-1]\n",
    "                    yn00_s = yn00_s.append(assign_dNdS(dN, dS, folder))\n",
    "                elif line.startswith('LWL85:') and 'nan' not in line:\n",
    "                    LWL85_s = LWL85_s.append(parse_dNdS_to_df(line, folder))\n",
    "                elif line.startswith('LWL85m:') and 'nan' not in line:\n",
    "                    LWL85m_s= LWL85m_s.append(parse_dNdS_to_df(line, folder))\n",
    "                elif line.startswith('LPB93:') and 'nan' not in line:\n",
    "                    LPB93_s =LPB93_s.append(parse_dNdS_to_df(line, folder))\n",
    "                else:\n",
    "                    continue\n",
    "    out_df = pd.concat([yn00_s.round(4),LWL85_s.round(4),\\\n",
    "                        LWL85m_s.round(4), LPB93_s.round(4)],axis =1)\n",
    "    new_columns = ['yn00_dN/dS','LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS' ]\n",
    "    if len(out_df) > 0:\n",
    "        out_df.rename(columns=dict(zip(out_df.columns, new_columns)), inplace=True)\n",
    "        out_name = os.path.join(tmp_path, '%s_%s.%s' % (out_df.index[0],out_df.index[-1],suffix))\n",
    "        out_df.to_csv(out_name, sep='\\t')\n",
    "    #return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:03.673311Z",
     "start_time": "2018-03-06T00:18:03.638967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkPamlFilesExist(alleleDf):\n",
    "    '''loops through all folder names in alleleDf.index to check if their PAML files have\n",
    "    all been generated in those folders. refDict is based on the contents of a folder\n",
    "    that was known to be run successfully.'''\n",
    "    refDict = {'aln': 2,\n",
    "     'clustal': 1,\n",
    "     'ctl': 1,\n",
    "     'dN': 1,\n",
    "     'dS': 1,\n",
    "     'fa': 2,\n",
    "     'out': 1,\n",
    "     'rst': 1,\n",
    "     'rst1': 1,\n",
    "     'rub': 1,\n",
    "     't': 1}\n",
    "    for file in (x for x in alleleDf.index if not pd.isnull(x)):\n",
    "        if not os.path.exists(os.path.join(PAML_PATH, file)):\n",
    "            return False\n",
    "        discrepancies = getDiscrepancies(os.path.join(PAML_PATH, file), refDict)\n",
    "        if discrepancies != '':\n",
    "            print(discrepancies)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:04.462433Z",
     "start_time": "2018-03-06T00:18:04.453162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return it.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:05.071017Z",
     "start_time": "2018-03-06T00:18:05.041145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineTmpToDf(header, suffix, tmp_path, clean=True):\n",
    "    \"\"\"Combines the files of a temporary folder into a dataframe based on the tmp files\n",
    "    suffix. Returns combined dataframe. And cleans up if needed.\"\"\"\n",
    "    tmp_assigneddfs_fh = [os.path.join(tmp_path, file) for file in os.listdir(tmp_path)\\\n",
    "                         if file.endswith(suffix) ]\n",
    "    print(len(tmp_assigneddfs_fh))\n",
    "    tmp_df = pd.DataFrame(columns=header)\n",
    "    for df_fh in tmp_assigneddfs_fh:\n",
    "        tmp_df = pd.concat([tmp_df, pd.read_csv(df_fh, index_col = 0, sep='\\t')])\n",
    "    if clean == True:\n",
    "        #now clean up again\n",
    "        for file in tmp_assigneddfs_fh:\n",
    "            os.remove(file)\n",
    "    return tmp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:05.782265Z",
     "start_time": "2018-03-06T00:18:05.721411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIdToLocusTagDict(gff_fh, kind):\n",
    "    '''function to generate a dictionary of key=ID and value=LocusTag from a gff file for\n",
    "    specific kind .e.g. mRNA, exon, gene.'''\n",
    "    gff_cnames = ['seqid', 'source', 'type', 'start', 'end', 'score', 'strand', 'phase', 'attributes']\n",
    "    df = pd.read_csv(gff_fh, sep='\\t', header=None, names=gff_cnames)\n",
    "    if kind not in ['gene',  'mRNA']:\n",
    "        print('this kind of feature is not supported. Please use e.g. gene or mRNA')\n",
    "        return False\n",
    "    \n",
    "    df = df[df['type'] == kind]\n",
    "    \n",
    "    locusSearch = re.compile(r'^.*locus_tag=(.*?)(;|$)')\n",
    "    idSearch = re.compile(r'ID=(.*?);')\n",
    "    \n",
    "    d = {}\n",
    "    \n",
    "    for attr in df['attributes']:\n",
    "        val = locusSearch.match(attr).group(1)\n",
    "        key = idSearch.match(attr).group(1)\n",
    "        if key in d.keys():\n",
    "            print(\"Unexpected: id key: %s already in dictionary.\" % key)\n",
    "        d[key] = val\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:06.532600Z",
     "start_time": "2018-03-06T00:18:06.515969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_count_singles(po_fh):\n",
    "    \"\"\"Count the singles in a po_fh file using * as counting thingy.\"\"\"\n",
    "    count = 0\n",
    "    with open(po_fh) as file:\n",
    "        for line in file:\n",
    "            if '*' in line:\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:07.170322Z",
     "start_time": "2018-03-06T00:18:07.156390Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_sequence_number(fh):\n",
    "    \"\"\"Counts the number of input fasta sequences\"\"\"\n",
    "    count = 0\n",
    "    with open(fh) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:10.140181Z",
     "start_time": "2018-03-06T00:18:08.016357Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % H_GENOME), header=0, sep='\\t')\n",
    "hFullAlleleDf['matchType'] = pd.Series([np.nan]*len(hFullAlleleDf.index), index=hFullAlleleDf.index)\n",
    "hFullAlleleDf['matchType'] = hFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)\n",
    "pFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % P_GENOME), header=0, sep='\\t')\n",
    "pFullAlleleDf['matchType'] = pd.Series([np.nan]*len(pFullAlleleDf.index), index=pFullAlleleDf.index)\n",
    "pFullAlleleDf['matchType'] = pFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:10.353289Z",
     "start_time": "2018-03-06T00:18:10.339594Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out haplotig proteins that already have alleles identified by BLAST or proteinortho.\n",
    "pFullAlleleDf['aQuery'] = pFullAlleleDf['Query']\n",
    "pFullAlleleDf['aTarget'] = pFullAlleleDf['Target']\n",
    "hFullAlleleDf['aQuery'] = hFullAlleleDf['Target']\n",
    "hFullAlleleDf['aTarget'] = hFullAlleleDf['Query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:11.244705Z",
     "start_time": "2018-03-06T00:18:11.161699Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#problem here is that adding a string to nan gives nan. Needs a function that converts the whole column to striing\n",
    "pFullAlleleDf['comp'] = pFullAlleleDf.aQuery.astype(str) + '_' + pFullAlleleDf.aTarget.astype(str)\n",
    "hFullAlleleDf['comp'] = hFullAlleleDf.aQuery.astype(str) + '_' + hFullAlleleDf.aTarget.astype(str)\n",
    "#now only add the hFullAlleleDf were the pairing of p and h blast hasn't been captured yet\n",
    "\n",
    "ahFullAlleleDf = hFullAlleleDf[~(hFullAlleleDf.comp.isin(pFullAlleleDf.comp))]\n",
    "\n",
    "\n",
    "#This change would require to search the unphased on the singles on aQuery and aTarget. It save on computing time\n",
    "#and potentially make downstream analysis easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:18:13.010114Z",
     "start_time": "2018-03-06T00:18:12.976697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phFullAlleleDf = pFullAlleleDf.append(ahFullAlleleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:20:32.322603Z",
     "start_time": "2018-03-06T00:20:27.563639Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQRECORD_PROTEIN_DICT = getFastaDict(PH_PROTEIN_FASTA)\n",
    "SEQRECORD_GENE_DICT = getFastaDict(PH_GENE_FASTA)\n",
    "SEQRECORD_CDS_DICT = getFastaDict(PH_CDS_FASTA)\n",
    "PROTEINID_LOCUSTAG_DICT = getIdToLocusTagDict(PH_GFF_DICT_FH, 'mRNA')\n",
    "PROTEINID_LOCUSTAG_DICT[np.nan] = np.nan #needed to add the nan key that nan \n",
    "#blast hits in aQuery/aTarget dont throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:20:47.112541Z",
     "start_time": "2018-03-06T00:20:47.021929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf = phFullAlleleDf.copy()\n",
    "alleleDf['folder'] = alleleDf.Query + '_' + alleleDf.Target #this relies on the fact that adding string to nan is nan\n",
    "alleleDf.set_index('folder', inplace=True)\n",
    "alleleDf['aQuery_LT'] = alleleDf.aQuery.apply(lambda x: PROTEINID_LOCUSTAG_DICT[x])\n",
    "alleleDf['aTarget_LT'] = alleleDf.aTarget.apply(lambda x: PROTEINID_LOCUSTAG_DICT[x])\n",
    "# assert(len(alleleDf) == len(overlapDf) + len(noOverlapDf) + len(diffContigDf) + len(manualAssignDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:20:49.774770Z",
     "start_time": "2018-03-06T00:20:49.217693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/gamran/genome_analysis/Warrior/Richard/scripts')\n",
    "%run file_counting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:20:51.036171Z",
     "start_time": "2018-03-06T00:20:50.672006Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(alleleDf = alleleDf):\n",
    "    prepareAlignmentBashScript(os.path.join(PAML_PATH, 'paml_script.sh'))\n",
    "    \n",
    "    # if already run before, comment out this line\n",
    "    print(\"Checking whether all PAML files already exist in %s...\" % PAML_PATH)\n",
    "    if checkPamlFilesExist(alleleDf):\n",
    "        print('PAML appears to have been run to completion previously. Therefore, it will not be run this time.')\n",
    "    else:\n",
    "        'Not all files generated by PAML appear to exist. Running PAML now (this may take some time)...'\n",
    "        !bash {os.path.join(PAML_PATH, 'paml_script.sh')}\n",
    "        print('PAML finished running.')\n",
    "\n",
    "    analysedAllelesPath = os.path.join(BASE_OUT_PATH, GENOME+'_analysed_alleles.df')    \n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    #dataframe where the index is not 'NaN'\n",
    "    noNANdf = alleleDf.loc[~alleleDf.index.isnull(),:].copy()\n",
    "    all_folders = noNANdf.index\n",
    "    #generate a tmp folder for the parallized analysis\n",
    "    tmp_path = os.path.join(BASE_OUT_PATH, 'tmp')\n",
    "    if not os.path.exists(tmp_path):\n",
    "        os.mkdir(tmp_path)\n",
    "    #assign the distances\n",
    "    dist_suffix = 'distdf_tmp'\n",
    "    #do parallized analysi\n",
    "    Parallel(n_jobs=threads)(delayed(assignDistancesToAllAlleles)(list(folder_index_list),all_folders,tmp_path, dist_suffix)\\\n",
    "                       for folder_index_list in grouper(noNANdf.index, 100, np.nan))\n",
    "    distdf_header = ['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    distdf = combineTmpToDf(distdf_header, dist_suffix, tmp_path, clean=True)\n",
    "    distdf['Index'] = distdf.index\n",
    "    noNANdf['Index'] = noNANdf.index\n",
    "    tmp_df = pd.merge(noNANdf, distdf,how='inner')\n",
    "    tmp_df.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    \n",
    "    print(\"Done with caculate pairwise alignment distances.\")\n",
    "    #pd.util.testing.assert_frame_equal(alleleDf, pd.read_csv(analysedAllelesPath, sep='\\t', index_col=0))\n",
    "    print(\"Starting to parse dN/dS ratios from yn.out file.\")\n",
    "    dNdS_suffix = 'dNdSdf_tmp'\n",
    "    #now assign the dNdS ratios\n",
    "    Parallel(n_jobs=threads)(delayed(assign_dNdS_to_all_alleles)(list(folder_index_list),all_folders,tmp_path, dNdS_suffix)\\\n",
    "                       for folder_index_list in grouper(noNANdf.index, 100, np.nan))\n",
    "    dNdS_header = ['yn00_dN/dS','LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS' ]\n",
    "    dNdSdf = combineTmpToDf(dNdS_header, dNdS_suffix, tmp_path, clean=True)\n",
    "    dNdSdf['Index'] = dNdSdf.index\n",
    "    tmp_df = pd.merge(tmp_df, dNdSdf,how='inner')\n",
    "    tmp_df.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    print('Finish to parse dN/dS ratios.')\n",
    "    print('Combining Dataframe of potential allele pairs with Dataframe of blast none hits.')\n",
    "    #now pull together the dataframes of hits and no-hits blast again.\n",
    "    pre_col_order = tmp_df.columns\n",
    "    alleleDf = pd.concat([tmp_df, alleleDf.loc[alleleDf.index.isnull(), :]], axis=0, ignore_index=True)\n",
    "    alleleDf = alleleDf.loc[:,pre_col_order]\n",
    "    \n",
    "    print(\"Assigning the unphased genes.\")\n",
    "    unphased_aQuery_s = assign_unphased(alleleDf.aQuery)\n",
    "    assert(any(unphased_aQuery_s.index == alleleDf.aQuery))\n",
    "    alleleDf['unphased_aQuery'] = unphased_aQuery_s.values\n",
    "    \n",
    "    \n",
    "    ####IN FUTURE INCOOPORATE THE FOLLOWING FOR H homozygous regions#####\n",
    "    #unphased_aTarget_s = assign_unphased(alleleDf.aTarget)\n",
    "    #assert(any(unphased_aTarget_s.index == alleleDf.aTarget))\n",
    "    #alleleDf['unphased_aTarget'] = unphased_aTarget_s.values\n",
    "    #\n",
    "    #\n",
    "\n",
    "    \n",
    "    print(\"Assining the proteinortho single genes.\")\n",
    "    PO_single_aQuery_s = assign_PO_single(alleleDf.aQuery)\n",
    "    assert(any(PO_single_aQuery_s.index == alleleDf.aQuery))\n",
    "    alleleDf['PO_single_aQuery'] = PO_single_aQuery_s.values\n",
    "    \n",
    "    PO_single_aTarget_s = assign_PO_single(alleleDf.aTarget)\n",
    "    assert(any(PO_single_aTarget_s.index == alleleDf.aTarget))\n",
    "    alleleDf['PO_single_aTarget'] = PO_single_aTarget_s.values\n",
    "    \n",
    "    \n",
    "    print(\"Assining the proteinortho inter haplome paralogoues by comparing\\\n",
    "    proteinortho (po) and its synteny mode output (poff)\")\n",
    "    alleleDf['aTarget+aQuery'] = alleleDf.aTarget + '+' + alleleDf.aQuery\n",
    "    interhaplotypeparaloges_s = assign_INP(alleleDf['aTarget+aQuery'])\n",
    "    assert(any(interhaplotypeparaloges_s.index == alleleDf['aTarget+aQuery'] ))\n",
    "    alleleDf['PO_interhaplotype_paralogs'] = interhaplotypeparaloges_s.values\n",
    "    alleleDf.drop('aTarget+aQuery', axis=1, inplace=True)\n",
    "    alleleDf.reset_index(drop=True, inplace=True)\n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:25:32.597712Z",
     "start_time": "2018-03-06T00:22:43.883234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether all PAML files already exist in /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/Warrior_comp_runs/allele_analysis/post_allele_analysis/paml...\n",
      "PAML appears to have been run to completion previously. Therefore, it will not be run this time.\n",
      "252\n",
      "Done with caculate pairwise alignment distances.\n",
      "Starting to parse dN/dS ratios from yn.out file.\n",
      "252\n",
      "Finish to parse dN/dS ratios.\n",
      "Combining Dataframe of potential allele pairs with Dataframe of blast none hits.\n",
      "Assigning the unphased genes.\n",
      "Assining the proteinortho single genes.\n",
      "Assining the proteinortho inter haplome paralogoues by comparing    proteinortho (po) and its synteny mode output (poff)\n",
      "Check poff and proteinortho output as conceptually all poff pairs should be in included in the proteinortho output.\n",
      "These are the pairs:\n",
      "['evm.model.hcontig_069_108.16+evm.model.pcontig_078.17', 'evm.model.hcontig_022_015.1+evm.model.pcontig_012.294']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    alleleDf_out = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T00:25:32.724688Z",
     "start_time": "2018-03-06T00:25:32.600365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////\n",
      "\n",
      "\n",
      "Appears all quick tests have completed successfully!\n",
      "\n",
      "//////\n"
     ]
    }
   ],
   "source": [
    "#now some quick tests in here\n",
    "assert(len(alleleDf) == len(alleleDf_out)) #in out out dataframe have the same length\n",
    "#we get the right amount of singles\n",
    "assert(\\\n",
    "       (alleleDf_out[(alleleDf_out.PO_single_aQuery == True)]['aQuery'].unique().shape[0]\\\n",
    "       + alleleDf_out[(alleleDf_out.PO_single_aTarget == True)]['aTarget'].unique().shape[0])\\\n",
    "       == expected_count_singles(proortho_fh))\n",
    "#queries and targets are the all included\n",
    "assert(alleleDf_out.aTarget.dropna().unique().shape[0] == get_input_sequence_number(H_PROTEIN_FASTA))\n",
    "assert(alleleDf_out.aQuery.dropna().unique().shape[0] == get_input_sequence_number(P_PROTEIN_FASTA))\n",
    "print('///////\\n\\n\\nAppears all quick tests have completed successfully!\\n\\n//////')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thoughts on how to parse this out or handle better:\n",
    "    * combine dataframes with a comp column so that repricocal hits are not done twice. \n",
    "    * use this compare column as indexing and for looping over things as well in case of pmal and distance \n",
    "    calculations\n",
    "    * calculate the unphased and singles on aTarget and aQuery indepedently\n",
    "    \n",
    "    This way would reduce the complexity of the analysis by making everything less redundant\n",
    "    \n",
    "Done.\n",
    "\n",
    "Write some test that confirm that data got pulled in correctly. E.g. all singles, all aTargets and aQueries.\n",
    "\n",
    "This is all done for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
