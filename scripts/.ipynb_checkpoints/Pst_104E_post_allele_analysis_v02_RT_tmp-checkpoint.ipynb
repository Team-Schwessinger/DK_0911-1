{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pst_104E_v12_post_allele_analysis_v02_RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on original code by Benjamin Schwessinger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: output from `Pst_104E_defining_alleles_v02` & primary+haplotig (ph) protein/gene/cds .*fasta* files.\n",
    "- Programs: **MUSCLE**, **PAML**\n",
    "- Purpose: generate and save a DataFrame containing dN/dS information (number of nonsynonymous substitutions per non-synonymous site to the number of synonymous substitutions per synonymous site), as well as Hamming & Levenshtein distances (measures of % identity). Also provides visualisations of some of this data.\n",
    "\n",
    "#### Overview\n",
    "1. Reads in the large allele DataFrames generated in `Pst_104E_defining_alleles_v02` (i.e. proteinortho hits OR best blast hit) - see description header cell in that notebook for more information on which alleles are included in that DataFrame.\n",
    "2. Filters the allele DataFrames based on %ID and %QCov (this can be set to filter only BLAST-identified alleles or both BLAST- and proteinortho-identified alleles) so that distance information is not calculated on an unnecessarily large number of alleles.\n",
    "3. Calculates distance & dN/dS information, and saves this to an output file so that it does not have to be re-calculated (if for whatever reason, the inputs change so that dN/dS or distance information should change, this output file (`xx_analysed_alleles.df`) should be deleted so that it can be re-generated.\n",
    "4. Plots graphs of allele-type distribution (pie chart) and allele-type Levenshtein distances (measures of similarity) for different levels of allele-filtering (QCov/TCov/%ID/Levenshtein similarity).\n",
    "\n",
    "NB:\n",
    "- dN/dS information is currently not utilised in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:00:33.402230Z",
     "start_time": "2018-02-27T06:00:32.707359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:00:35.055027Z",
     "start_time": "2018-02-27T06:00:33.419537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import distance\n",
    "import editdistance\n",
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import collections\n",
    "import pybedtools\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:00:35.127203Z",
     "start_time": "2018-02-27T06:00:35.067270Z"
    }
   },
   "outputs": [],
   "source": [
    "GENOME_VERSION = 'Pst_104E_v12'\n",
    "\n",
    "BASE_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/Warrior_comp_runs/allele_analysis'\n",
    "\n",
    "YN00_PATH = os.path.join(BASE_PATH, 'post_allele_analysis', 'yn00.ctl')\n",
    "BASE_OUT_PATH = os.path.join(BASE_PATH, 'post_allele_analysis')\n",
    "ALLELE_PATH = os.path.join(BASE_PATH,'defining_alleles/allele_analysis/alleles_proteinortho_graph516' )\n",
    "UNFILTERED_DF_PATH = os.path.join(BASE_PATH,'defining_alleles/allele_analysis',\\\n",
    "                '%s_p_ctg.%s_h_ctg.0.001.blastp.outfmt6.allele_analysis' % (GENOME_VERSION, GENOME_VERSION))\n",
    "GENOME_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'\n",
    "FIGURE_PATH = os.path.join(BASE_OUT_PATH, 'figures')\n",
    "\n",
    "GENOME = GENOME_VERSION\n",
    "P_GENOME = GENOME + '_p_ctg'\n",
    "H_GENOME = GENOME + '_h_ctg'\n",
    "\n",
    "\n",
    "# Base filtering so that distance calculations are not performed on all allele pairs.\n",
    "# Distance calculations will only be performed on allele pairs above the defined cutoffs.\n",
    "# Note that proteinortho alleles will not be affected (this can be changed in the filterAlleleDf function).\n",
    "BASE_QCOV_CUTOFF = 0\n",
    "BASE_TCOV_CUTOFF = 0\n",
    "BASE_PCTID_CUTOFF = 0\n",
    "\n",
    "P_PROTEINS_FASTA = os.path.join(GENOME_PATH, P_GENOME + '.anno.protein.fa')\n",
    "PH_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.protein.fa')\n",
    "PH_GENE_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.gene.fa')\n",
    "PH_CDS_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.CDS.fa')\n",
    "\n",
    "PAML_PATH = os.path.join(BASE_OUT_PATH, 'paml')\n",
    "if not os.path.exists(BASE_OUT_PATH):\n",
    "    os.mkdir(BASE_OUT_PATH)\n",
    "if not os.path.exists(FIGURE_PATH):\n",
    "    os.mkdir(FIGURE_PATH)\n",
    "if not os.path.exists(PAML_PATH):\n",
    "    os.mkdir(PAML_PATH)\n",
    "    shutil.copy2(YN00_PATH, PAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:04:50.538095Z",
     "start_time": "2018-02-27T06:04:50.527565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/Warrior_comp_runs/allele_analysis'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:03:06.089773Z",
     "start_time": "2018-02-27T06:03:06.081494Z"
    }
   },
   "outputs": [],
   "source": [
    "#for homozygous region analysis\n",
    "COV_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/COV'\n",
    "homo_bed_fh = os.path.join(COV_PATH, 'Pst_104E_v12_ph_ctg.ph_p_homo_cov.bed')\n",
    "anno_gff_p_fh = os.path.join(GENOME_PATH, 'Pst_104E_v12_p_ctg.sorted.anno.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:04:31.382982Z",
     "start_time": "2018-02-27T06:04:31.375718Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PH_PROTEIN_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.protein.fa')\n",
    "PH_GENE_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.gene.fa')\n",
    "PH_CDS_FASTA = os.path.join(GENOME_PATH, GENOME + '_ph_ctg.anno.CDS.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:48.254045Z",
     "start_time": "2018-02-27T06:05:48.239449Z"
    }
   },
   "outputs": [],
   "source": [
    "#for finalizing the protein ortho single analysis\n",
    "proortho_fh = os.path.join(BASE_PATH, 'defining_alleles','proteinortho', 'ph_ctg_516.proteinortho')\n",
    "proorthograph_fh = os.path.join(BASE_PATH,'defining_alleles', 'proteinortho', 'ph_ctg_516.proteinortho-graph')\n",
    "poff_graph_fh = os.path.join(BASE_PATH, 'defining_alleles','proteinortho', 'ph_ctg_516.poff-graph')\n",
    "assert(os.path.exists(proortho_fh))\n",
    "assert(os.path.exists(proorthograph_fh))\n",
    "assert(os.path.exists(poff_graph_fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:50.690135Z",
     "start_time": "2018-02-27T06:05:50.632933Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInterHaplotypeParaloges(proorthograph_fh,poff_graph_fh):\n",
    "    \"\"\"This function compares the proteinorth (po) and po synteny output. \n",
    "    It returns all Target+Query pairs that are only found in po as list. \n",
    "    It also checks if there are pairs only found in poff.\"\"\"\n",
    "    graph_header = ['Target', 'Query', 'evalue_ab', 'bitscore_ab', 'evalue_ba', 'bitscore_ba', 'same_strand' , 'simscore']\n",
    "    poff_df = pd.read_csv(poff_graph_fh, sep='\\t', header=None, names=graph_header, comment='#' )\n",
    "    po_df = pd.read_csv(proorthograph_fh, sep='\\t', header=None, names=graph_header, comment='#' )\n",
    "    poff_paring = (poff_df.Target+ '+' + poff_df.Query)\n",
    "    po_paring = (po_df.Target+ '+' + po_df.Query)\n",
    "    interhaplotypeparaloges_l = list(set(po_paring) - set(poff_paring))\n",
    "    poff_only_pairs = list(set(poff_paring) - set(po_paring))\n",
    "    if len(poff_only_pairs) > 0:\n",
    "        print(\"Check poff and proteinortho output as conceptually all poff pairs should be in included in the proteinortho output.\\nThese are the pairs:\")\n",
    "        print(poff_only_pairs)\n",
    "    return interhaplotypeparaloges_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:51.054188Z",
     "start_time": "2018-02-27T06:05:51.023436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_INP(aTargetplusaQuery, proorthograph_fh = proorthograph_fh,poff_graph_fh = poff_graph_fh ):\n",
    "    \"\"\"\n",
    "    Returns a series of of True/False values if a aTarget+aQuery pair are element identified as \n",
    "    Inter haplotype paraloge.\n",
    "    Input:\n",
    "        proorthograh_fh, proteinortho graph output file handle that has been run including the single flag\n",
    "        queries, series of protein queries used for the blast search.\n",
    "        poff_graph_fh, proteinortho graph output file handle that has been run including the synteny and\n",
    "        single flag\n",
    "        aTargetplusaQuery, series of protein aTargets + aQuiers used for the blast search.\n",
    "    Outupt:\n",
    "        Returns a True/False series with the aTargets + aQuiers as index.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    interhaplotypeparaloges_l = getInterHaplotypeParaloges(proorthograph_fh,poff_graph_fh)\n",
    "    interhaplotypeparaloges_s = pd.Series([False]*len(aTargetplusaQuery), index=aTargetplusaQuery)\n",
    "    interhaplotypeparaloges_s.loc[interhaplotypeparaloges_s.index.isin(interhaplotypeparaloges_l)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return interhaplotypeparaloges_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:51.698192Z",
     "start_time": "2018-02-27T06:05:51.671460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProOrthsingles(proortho_fh):\n",
    "    \"\"\"Parses out all proteinortho singles from a proteinortho output file.\n",
    "    Returns a list of these singles.\"\"\"\n",
    "    with open(proortho_fh) as file:\n",
    "        id_pattern = r'(evm\\S*)'\n",
    "        regex = re.compile(id_pattern)\n",
    "        po_single_list = []\n",
    "        for line in file:\n",
    "            if '*' in line:\n",
    "                po_single_list.append(regex.search(line).groups()[0])\n",
    "            else:\n",
    "                continue\n",
    "    return po_single_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:52.298313Z",
     "start_time": "2018-02-27T06:05:52.279412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_PO_single(queries, proortho_fh=proortho_fh):\n",
    "    \"\"\"\n",
    "    Returns a series of of True/False values if a query element identified as single.\n",
    "    Input:\n",
    "        proortho_fh, proteinortho output file handle that has been run including the single flag\n",
    "        queries, series of protein queries used for the blast search.\n",
    "    Outupt:\n",
    "        Returns a True/False series with the queries as index.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    PO_singles = getProOrthsingles(proortho_fh)\n",
    "    PO_singles_s = pd.Series([False]*len(queries), index=queries)\n",
    "    PO_singles_s.loc[PO_singles_s.index.isin(PO_singles)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return PO_singles_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:52.855276Z",
     "start_time": "2018-02-27T06:05:52.843523Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allgffTogenegff(gff_fh, write_out=True):\n",
    "    '''Converts at complete gff to a gene gff only and writtes it out.'''\n",
    "    gene_gff = pd.read_csv(gff_fh, sep='\\t', header=None)\n",
    "    gene_gff = gene_gff[gene_gff[2] == 'gene']\n",
    "    gene_gff.reset_index(drop=True, inplace=True)\n",
    "    gene_gff.to_csv(gff_fh.replace('anno', 'gene'), sep='\\t', header=False, index=False)\n",
    "    return gene_gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:53.435561Z",
     "start_time": "2018-02-27T06:05:53.415273Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_8_id(x):\n",
    "    '''Function that pulls out the ID from the 9th column of a df.'''\n",
    "    pattern = r'ID=([a-zA-Z0-9_.]*);'\n",
    "    regex = re.compile(pattern)  \n",
    "    m = regex.search(x)\n",
    "    match = m.groups()[0].replace('TU', 'model')\n",
    "    if match.startswith('cds.'):\n",
    "        match = match[4:]\n",
    "    if 'exon' in match:\n",
    "        _list = match.split('.')\n",
    "        match = '.'.join(_list[:-1])\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:54.241130Z",
     "start_time": "2018-02-27T06:05:54.157206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignMatchType(allele_source, overlap, no_overlap):\n",
    "    if pd.isnull(allele_source):\n",
    "        return allele_source\n",
    "    \n",
    "    s = allele_source + '_'\n",
    "    \n",
    "    if overlap ==  True:\n",
    "        s += 'overlap'\n",
    "    elif no_overlap == True:\n",
    "        s += 'no_overlap'\n",
    "    else: # different_pcontig\n",
    "        s += 'unlinked'\n",
    "    return s\n",
    "\n",
    "def reduceGroups(g):\n",
    "    '''returns the best hit based on e-value and BitScore per group'''\n",
    "    if len(g) == 1:\n",
    "        return g\n",
    "    tmp_g = g[g['e-value'] == g['e-value'].min()]\n",
    "    if len(tmp_g) == 1:\n",
    "        return tmp_g\n",
    "    return tmp_g[tmp_g['BitScore'] == tmp_g['BitScore'].max()]\n",
    "\n",
    "def filterAlleleDf(alleleDf, qCov, tCov, pctId, levSim, leavePO=False):\n",
    "    if leavePO:\n",
    "        no_PO_df = alleleDf[(alleleDf['allele_source'] == 'h_rBLAST') | (alleleDf['allele_source'] == 'BLAST')]\n",
    "        PO_df = alleleDf[alleleDf['allele_source'] == 'PO']\n",
    "\n",
    "        filtered_no_PO_df = filterAlleleDf(no_PO_df, qCov, tCov, pctId, levSim)\n",
    "        return filtered_no_PO_df.append(PO_df, ignore_index=True)\n",
    "    \n",
    "    if qCov:\n",
    "        alleleDf = alleleDf[alleleDf['QCov'] > qCov]\n",
    "    if tCov:\n",
    "        alleleDf = alleleDf[alleleDf['TCov'] > tCov]\n",
    "    if pctId:\n",
    "        alleleDf = alleleDf[alleleDf['PctID'] > pctId]\n",
    "    if levSim:\n",
    "        levDist = (100-levSim)/100.0\n",
    "        alleleDf = alleleDf[alleleDf['protein_levenshtein'] < levDist]\n",
    "\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:54.894130Z",
     "start_time": "2018-02-27T06:05:54.875545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geneUnphased(Gene_gff_fh, Homo_cov_bed_fh ):\n",
    "    \"\"\"\n",
    "    Returns a list of all genes that are unphased.\n",
    "    \n",
    "    Input: * Fh for annotation gff file\n",
    "           * Fh for Homo_cov_bed_fh\n",
    "    Output: A set of gene IDs that are unphases\n",
    "    \"\"\"\n",
    "    geneGff_bed = pybedtools.BedTool(Gene_gff_fh)\n",
    "    homo_p_bed = pybedtools.BedTool(Homo_cov_bed_fh)\n",
    "    gene_ids_ph_p_homo = []\n",
    "    for x in geneGff_bed.intersect(homo_p_bed, f=0.4):\n",
    "        y = col_8_id(x[8])\n",
    "        gene_ids_ph_p_homo.append(y)\n",
    "    gene_ids_ph_p_homo = set(gene_ids_ph_p_homo)\n",
    "    return gene_ids_ph_p_homo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:55.550040Z",
     "start_time": "2018-02-27T06:05:55.531547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_unphased(queries, gff_fh=anno_gff_p_fh, homo_bed_fh=homo_bed_fh):\n",
    "    \"\"\"\n",
    "    Takes a series of gene models and checks if they overlap with the\n",
    "    homo_bed dataframe. The minimum overlap is f=0.4 with the intersect function.\n",
    "    \n",
    "    TO DO: For now the homo_bed only includes primary gene models. Should be changed.\n",
    "    Input:  \n",
    "        queries, pd.Series of gene names. \n",
    "        gff_fh, file handle of gff file.\n",
    "        homo_bed_dfh, file handle of bed file that represents non-phased regions.\n",
    "    \n",
    "    \"\"\"\n",
    "    if '.gene' not in gff_fh: \n",
    "        _ = allgffTogenegff(gff_fh)\n",
    "    Unphasedgenes = geneUnphased(gff_fh.replace('anno', 'gene'), homo_bed_fh)\n",
    "    unphased_s = pd.Series([False]*len(queries), index=queries)\n",
    "    unphased_s.loc[unphased_s.index.isin(Unphasedgenes)] = True\n",
    "    #print(len(Unphasedgenes))\n",
    "    return unphased_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:56.202074Z",
     "start_time": "2018-02-27T06:05:56.191435Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFastaDict(fastaFile):\n",
    "    d = {}\n",
    "    for gene in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        d[gene.id] = gene\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:05:58.183443Z",
     "start_time": "2018-02-27T06:05:58.127611Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeAllelicFasta(alleleOne, alleleTwo, alleleType, outPath):\n",
    "    '''writes fasta file containing fasta information for two alleles\n",
    "    in the outPath'''\n",
    "    assert(alleleType.upper() in ['CDS', 'GENE', 'PROTEIN'])\n",
    "    \n",
    "    seqRecordDict = globals()['SEQRECORD_' + alleleType.upper() + '_DICT']\n",
    "    try:\n",
    "        alleleSeqRecords = [seqRecordDict[alleleOne], seqRecordDict[alleleTwo]]\n",
    "    except KeyError:\n",
    "        print(alleleOne)\n",
    "        print(alleleTwo)\n",
    "        print(alleleType)\n",
    "        sys.exit()\n",
    "    with open(os.path.join(outPath, alleleType.lower() + '.fa'), 'w') as outFile:\n",
    "        SeqIO.write(alleleSeqRecords, outFile, 'fasta')\n",
    "    return True\n",
    "\n",
    "def writeAlignmentScript(alleleOutPath, scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'a') as outFile:\n",
    "        print('cd %s' % alleleOutPath, file=outFile)\n",
    "        print('/home/gamran/anaconda3/muscle3.8.31_i86linux64 -clwstrict -in protein.fa -out protein.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl -output paml protein.aln cds.fa > cds_codon.aln', file=outFile)\n",
    "        print('perl /home/gamran/anaconda3/pal2nal.v14/pal2nal.pl protein.aln cds.fa > cds_codon.clustal', file=outFile)\n",
    "        print('cp %s/yn00.ctl ./' % PAML_PATH, file=outFile)\n",
    "        print('/home/gamran/anaconda3/paml4.9g/bin/yn00', file=outFile)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:01.458015Z",
     "start_time": "2018-02-27T06:06:01.423526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareAlignmentBashScript(scriptLoc = os.path.join(PAML_PATH, 'paml_script.sh')):\n",
    "    with open(scriptLoc, 'w') as pamlScript:\n",
    "        print('#!/bin/bash', file=pamlScript)\n",
    "\n",
    "    for index, [Query, Target] in alleleDf.iloc[:, :2].iterrows():\n",
    "        #if we don't have a blast hit skip.\n",
    "        if pd.isnull(Target):\n",
    "            continue\n",
    "        else:\n",
    "            alleleOutPath = os.path.join(PAML_PATH, '%s_%s' % (Query, Target))\n",
    "            if not os.path.exists(alleleOutPath):\n",
    "                os.mkdir(os.path.join(PAML_PATH, '%s_%s' % (Query, Target)))\n",
    "\n",
    "            writeAllelicFasta(Query, Target, 'CDS', alleleOutPath)\n",
    "            writeAllelicFasta(Query, Target, 'PROTEIN', alleleOutPath)\n",
    "\n",
    "            writeAlignmentScript(alleleOutPath, os.path.join(PAML_PATH, 'paml_script.sh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:02.374032Z",
     "start_time": "2018-02-27T06:06:02.227083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignDistancesToAlleles(folder, alignmentFile, alleleType):\n",
    "    '''Adds Hamming and Levenshtein distance columns to an allele pair\n",
    "    (indexed by 'folder' name) in df'''\n",
    "    #print(folder)\n",
    "    if pd.isnull(folder):\n",
    "        return np.nan, np.nan\n",
    "    assert(alleleType.upper() in ['PROTEIN', 'CDS', 'GENE'])\n",
    "    seq1, seq2 = AlignIO.read(open(alignmentFile, 'r'), format='clustal', seq_count=2)\n",
    "    seq1 = str(seq1.seq).upper()\n",
    "    seq2 = str(seq2.seq).upper()\n",
    "    assert(len(seq1) == len(seq2))\n",
    "    return editdistance.eval(seq1, seq2)/len(seq1), distance.hamming(seq1, seq2, normalized=True)\n",
    "\n",
    "def assignDistancesToAllAlleles(df_folder_index, all_folders, tmp_path, suffix):\n",
    "    \"\"\"\n",
    "    Reads in the index that contains the folder pairings for the alignements.\n",
    "    Returns a protein_df and CDS_df that contain the hamming and levenshtein distance each.\n",
    "    \"\"\"\n",
    "    cleaned_index = [x for x in df_folder_index if x in all_folders]\n",
    "    count = 0\n",
    "    total = len(df_folder_index)\n",
    "    percentDone = 0\n",
    "    protein_lev_dict = {}\n",
    "    protein_ham_dict = {}\n",
    "    CDS_lev_dict = {}\n",
    "    CDS_ham_dict = {}\n",
    "    \n",
    "    #print(\"Calculating distances and adding them to the allele DataFrame...\")\n",
    "    \n",
    "    for folder in cleaned_index:\n",
    "        if pd.isnull(folder):\n",
    "            proteinAlignmentFile = ''\n",
    "            cdsAlignmentFile = ''\n",
    "        else:\n",
    "            proteinAlignmentFile = os.path.join(PAML_PATH, folder, 'protein.aln')\n",
    "            cdsAlignmentFile = os.path.join(PAML_PATH, folder, 'cds_codon.clustal')\n",
    "        #here the nan get overwritten. This doesn't matter though as they are all\n",
    "        #nan anyway.\n",
    "        protein_lev_dict[folder], protein_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, proteinAlignmentFile, 'PROTEIN')\n",
    "        CDS_lev_dict[folder], CDS_ham_dict[folder]  = \\\n",
    "        assignDistancesToAlleles(folder, cdsAlignmentFile, 'CDS')\n",
    "\n",
    "        count += 1\n",
    "        #if round(count/total * 100) > percentDone:\n",
    "            #percentDone = round(count/total * 100)\n",
    "            #print(\"%s%% complete\" % percentDone)\n",
    "            \n",
    "    newdf_columns=['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    if len(protein_ham_dict) > 0:\n",
    "        df = pd.DataFrame([protein_ham_dict,protein_lev_dict,CDS_ham_dict,CDS_lev_dict]).T\n",
    "        df.rename(columns=dict(zip(df.columns,newdf_columns)),inplace=True)\n",
    "        out_name = os.path.join(tmp_path, '%s_%s.%s' % (df.index[0],df.index[-1],suffix))\n",
    "        df.round(4).to_csv(out_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:03.170070Z",
     "start_time": "2018-02-27T06:06:03.011517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dNdS_to_df(line, folder):\n",
    "    \"\"\"\n",
    "    Function that parses out dN and dS of a yn00 file and calls the \n",
    "    assign_dNdS function. Therefore returns a single element\n",
    "    pd.Series with folder name as index.\n",
    "    \"\"\"\n",
    "    dN = re.findall(r'dN = [-| ]?(.*) w', line)[0]\n",
    "    dS = re.findall(r'dS = [-| ]?(.*) dN', line)[0]\n",
    "    return assign_dNdS(dN, dS, folder)\n",
    "\n",
    "def assign_dNdS(dN, dS, folder):\n",
    "    '''\n",
    "    Function that cacluates the dN/dS ratio an returns it as a series usind the folder as index.\n",
    "    Input: dN, dS, folder(name)\n",
    "    Output: single element pd.Series with folder name as index.\n",
    "    '''\n",
    "    if float(dS) > 0:\n",
    "        series = pd.Series([float(dN)/float(dS)], index=[folder])\n",
    "    else:\n",
    "        series = pd.Series([np.nan], index=[folder])\n",
    "    return series\n",
    "\n",
    "def assign_dNdS_to_all_alleles(folder_index, all_folders, tmp_path, suffix):\n",
    "    \"\"\"Function that parses out the different dN/dS ratios from a yn.out file for a list/index\n",
    "    of folders that contain these yn.out files. The output dataframe is saved to tmp folder using\n",
    "    the suffix.\n",
    "    Input:  folder_index, list or index where to find the yn.out for each pairing.\n",
    "            all_folders, are all possible folders. This is used to filter out nan and so from\n",
    "                paralizing etc.\n",
    "            tmp_path, is the path were \n",
    "    Output: Saved out tmp df with the suffix as file ending.\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_index = [x for x in folder_index if x in all_folders]\n",
    "    #print(cleaned_index)\n",
    "    yn00_s = pd.Series([], name='yn00_dN/dS')\n",
    "    LWL85_s = pd.Series([], name='LWL85_dN/dS')\n",
    "    LWL85m_s = pd.Series([], name='LWL85m_dN/dS')\n",
    "    LPB93_s = pd.Series([], name='LPB93_dN/dS')\n",
    "    #header = ['folder','yn00_dN/dS', 'LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS']\n",
    "    #append these list\n",
    "    for folder in cleaned_index:\n",
    "        alleleYn = os.path.join(PAML_PATH, folder,'yn.out')\n",
    "        with open(alleleYn, 'r') as ynOut:\n",
    "            #now loop over the lines and parse out stuff\n",
    "            for i, line in enumerate(ynOut):\n",
    "                if line.startswith('seq. seq. ') and i > 0:\n",
    "                    next(ynOut) # we want the line that is two after the line starting with 'seq. seq '\n",
    "                    dataLine = next(ynOut)\n",
    "                    dN = dataLine.split('+-')[0].rstrip().split(' ')[-1]\n",
    "                    dS = dataLine.split('+-')[1].rstrip().split(' ')[-1]\n",
    "                    yn00_s = yn00_s.append(assign_dNdS(dN, dS, folder))\n",
    "                elif line.startswith('LWL85:') and 'nan' not in line:\n",
    "                    LWL85_s = LWL85_s.append(parse_dNdS_to_df(line, folder))\n",
    "                elif line.startswith('LWL85m:') and 'nan' not in line:\n",
    "                    LWL85m_s= LWL85m_s.append(parse_dNdS_to_df(line, folder))\n",
    "                elif line.startswith('LPB93:') and 'nan' not in line:\n",
    "                    LPB93_s =LPB93_s.append(parse_dNdS_to_df(line, folder))\n",
    "                else:\n",
    "                    continue\n",
    "    out_df = pd.concat([yn00_s.round(4),LWL85_s.round(4),\\\n",
    "                        LWL85m_s.round(4), LPB93_s.round(4)],axis =1)\n",
    "    new_columns = ['yn00_dN/dS','LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS' ]\n",
    "    if len(out_df) > 0:\n",
    "        out_df.rename(columns=dict(zip(out_df.columns, new_columns)), inplace=True)\n",
    "        out_name = os.path.join(tmp_path, '%s_%s.%s' % (out_df.index[0],out_df.index[-1],suffix))\n",
    "        out_df.to_csv(out_name, sep='\\t')\n",
    "    #return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:04.050013Z",
     "start_time": "2018-02-27T06:06:04.020333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkPamlFilesExist(alleleDf):\n",
    "    '''loops through all folder names in alleleDf.index to check if their PAML files have\n",
    "    all been generated in those folders. refDict is based on the contents of a folder\n",
    "    that was known to be run successfully.'''\n",
    "    refDict = {'aln': 2,\n",
    "     'clustal': 1,\n",
    "     'ctl': 1,\n",
    "     'dN': 1,\n",
    "     'dS': 1,\n",
    "     'fa': 2,\n",
    "     'out': 1,\n",
    "     'rst': 1,\n",
    "     'rst1': 1,\n",
    "     'rub': 1,\n",
    "     't': 1}\n",
    "    for file in (x for x in alleleDf.index if not pd.isnull(x)):\n",
    "        if not os.path.exists(os.path.join(PAML_PATH, file)):\n",
    "            return False\n",
    "        discrepancies = getDiscrepancies(os.path.join(PAML_PATH, file), refDict)\n",
    "        if discrepancies != '':\n",
    "            print(discrepancies)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:04.802004Z",
     "start_time": "2018-02-27T06:06:04.785717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return it.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:05.549799Z",
     "start_time": "2018-02-27T06:06:05.529798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineTmpToDf(header, suffix, tmp_path, clean=True):\n",
    "    \"\"\"Combines the files of a temporary folder into a dataframe based on the tmp files\n",
    "    suffix. Returns combined dataframe. And cleans up if needed.\"\"\"\n",
    "    tmp_assigneddfs_fh = [os.path.join(tmp_path, file) for file in os.listdir(tmp_path)\\\n",
    "                         if file.endswith(suffix) ]\n",
    "    print(len(tmp_assigneddfs_fh))\n",
    "    tmp_df = pd.DataFrame(columns=header)\n",
    "    for df_fh in tmp_assigneddfs_fh:\n",
    "        tmp_df = pd.concat([tmp_df, pd.read_csv(df_fh, index_col = 0, sep='\\t')])\n",
    "    if clean == True:\n",
    "        #now clean up again\n",
    "        for file in tmp_assigneddfs_fh:\n",
    "            os.remove(file)\n",
    "    return tmp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:12.442221Z",
     "start_time": "2018-02-27T06:06:10.415493Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % H_GENOME), header=0, sep='\\t')\n",
    "hFullAlleleDf['matchType'] = pd.Series([np.nan]*len(hFullAlleleDf.index), index=hFullAlleleDf.index)\n",
    "hFullAlleleDf['matchType'] = hFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)\n",
    "pFullAlleleDf = pd.read_csv(os.path.join(ALLELE_PATH, '%s.full_df.alleles' % P_GENOME), header=0, sep='\\t')\n",
    "pFullAlleleDf['matchType'] = pd.Series([np.nan]*len(pFullAlleleDf.index), index=pFullAlleleDf.index)\n",
    "pFullAlleleDf['matchType'] = pFullAlleleDf.apply(lambda row: assignMatchType(row['allele_source'], row['t_contig == h_contig_overlap'], row['q_contig == t_contig']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:12.462023Z",
     "start_time": "2018-02-27T06:06:12.445153Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out haplotig proteins that already have alleles identified by BLAST or proteinortho.\n",
    "#hFullAlleleDf = hFullAlleleDf[(~hFullAlleleDf['Query'].isin(pFullAlleleDf['Target']))]\n",
    "pFullAlleleDf['aQuery'] = pFullAlleleDf['Query']\n",
    "pFullAlleleDf['aTarget'] = pFullAlleleDf['Target']\n",
    "hFullAlleleDf['aQuery'] = hFullAlleleDf['Target']\n",
    "hFullAlleleDf['aTarget'] = hFullAlleleDf['Query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:12.510096Z",
     "start_time": "2018-02-27T06:06:12.464028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phFullAlleleDf = pFullAlleleDf.append(hFullAlleleDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:16.306118Z",
     "start_time": "2018-02-27T06:06:12.527227Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQRECORD_PROTEIN_DICT = getFastaDict(PH_PROTEIN_FASTA)\n",
    "SEQRECORD_GENE_DICT = getFastaDict(PH_GENE_FASTA)\n",
    "SEQRECORD_CDS_DICT = getFastaDict(PH_CDS_FASTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:16.510990Z",
     "start_time": "2018-02-27T06:06:16.439166Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf = phFullAlleleDf.copy()\n",
    "alleleDf['folder'] = alleleDf.Query + '_' + alleleDf.Target\n",
    "alleleDf.set_index('folder', inplace=True)\n",
    "# assert(len(alleleDf) == len(overlapDf) + len(noOverlapDf) + len(diffContigDf) + len(manualAssignDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:18.045454Z",
     "start_time": "2018-02-27T06:06:17.775279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/gamran/genome_analysis/Warrior/Richard/scripts')\n",
    "%run file_counting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T06:06:19.154061Z",
     "start_time": "2018-02-27T06:06:18.879544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(alleleDf = alleleDf):\n",
    "    prepareAlignmentBashScript(os.path.join(PAML_PATH, 'paml_script.sh'))\n",
    "    \n",
    "    # if already run before, comment out this line\n",
    "    print(\"Checking whether all PAML files already exist in %s...\" % PAML_PATH)\n",
    "    if checkPamlFilesExist(alleleDf):\n",
    "        print('PAML appears to have been run to completion previously. Therefore, it will not be run this time.')\n",
    "    else:\n",
    "        'Not all files generated by PAML appear to exist. Running PAML now (this may take some time)...'\n",
    "        !bash {os.path.join(PAML_PATH, 'paml_script.sh')}\n",
    "        print('PAML finished running.')\n",
    "\n",
    "    analysedAllelesPath = os.path.join(BASE_OUT_PATH, GENOME+'_analysed_alleles.df')    \n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    #dataframe where the index is not 'NaN'\n",
    "    noNANdf = alleleDf.loc[~alleleDf.index.isnull(),:].copy()\n",
    "    all_folders = noNANdf.index\n",
    "    #generate a tmp folder for the parallized analysis\n",
    "    tmp_path = os.path.join(BASE_OUT_PATH, 'tmp')\n",
    "    if not os.path.exists(tmp_path):\n",
    "        os.mkdir(tmp_path)\n",
    "    #assign the distances\n",
    "    dist_suffix = 'distdf_tmp'\n",
    "    #do parallized analysi\n",
    "    Parallel(n_jobs=threads)(delayed(assignDistancesToAllAlleles)(list(folder_index_list),all_folders,tmp_path, dist_suffix)\\\n",
    "                       for folder_index_list in grouper(noNANdf.index, 100, np.nan))\n",
    "    distdf_header = ['protein_hamming', 'protein_levenshtein', 'cds_hamming',\n",
    "       'cds_levenshtein']\n",
    "    distdf = combineTmpToDf(distdf_header, dist_suffix, tmp_path, clean=True)\n",
    "    distdf['Index'] = distdf.index\n",
    "    noNANdf['Index'] = noNANdf.index\n",
    "    tmp_df = pd.merge(noNANdf, distdf,how='inner')\n",
    "    tmp_df.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    \n",
    "    print(\"Done with caculate pairwise alignment distances.\")\n",
    "    #pd.util.testing.assert_frame_equal(alleleDf, pd.read_csv(analysedAllelesPath, sep='\\t', index_col=0))\n",
    "    print(\"Starting to parse dN/dS ratios from yn.out file.\")\n",
    "    dNdS_suffix = 'dNdSdf_tmp'\n",
    "    #now assign the dNdS ratios\n",
    "    Parallel(n_jobs=threads)(delayed(assign_dNdS_to_all_alleles)(list(folder_index_list),all_folders,tmp_path, dNdS_suffix)\\\n",
    "                       for folder_index_list in grouper(noNANdf.index, 100, np.nan))\n",
    "    dNdS_header = ['yn00_dN/dS','LWL85_dN/dS','LWL85m_dN/dS','LPB93_dN/dS' ]\n",
    "    dNdSdf = combineTmpToDf(dNdS_header, dNdS_suffix, tmp_path, clean=True)\n",
    "    dNdSdf['Index'] = dNdSdf.index\n",
    "    tmp_df = pd.merge(tmp_df, dNdSdf,how='inner')\n",
    "    tmp_df.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    print('Finish to parse dN/dS ratios.')\n",
    "    print('Combining Dataframe of potential allele pairs with Dataframe of blast none hits.')\n",
    "    #now pull together the dataframes of hits and no-hits blast again.\n",
    "    pre_col_order = tmp_df.columns\n",
    "    alleleDf = pd.concat([tmp_df, alleleDf.loc[alleleDf.index.isnull(), :]], axis=0, ignore_index=True)\n",
    "    alleleDf = alleleDf.loc[:,pre_col_order]\n",
    "    \n",
    "    print(\"Assigning the unphased genes.\")\n",
    "    unphased_s = assign_unphased(alleleDf.Query)\n",
    "    assert(any(unphased_s.index == alleleDf.Query))\n",
    "    alleleDf['unphased'] = unphased_s.values\n",
    "    \n",
    "    print(\"Assining the proteinortho single genes.\")\n",
    "    PO_single_s = assign_PO_single(alleleDf.Query)\n",
    "    assert(any(PO_single_s.index == alleleDf.Query))\n",
    "    alleleDf['PO_single'] = PO_single_s.values\n",
    "    \n",
    "    print(\"Assining the proteinortho inter haplome paralogoues by comparing\\\n",
    "    proteinortho (po) and its synteny mode output (poff)\")\n",
    "    alleleDf['aTarget+aQuery'] = alleleDf.aTarget + '+' + alleleDf.aQuery\n",
    "    interhaplotypeparaloges_s = assign_INP(alleleDf['aTarget+aQuery'])\n",
    "    assert(any(interhaplotypeparaloges_s.index == alleleDf['aTarget+aQuery'] ))\n",
    "    alleleDf['PO_interhaplotype_paralogs'] = interhaplotypeparaloges_s.values\n",
    "    alleleDf.drop('aTarget+aQuery', axis=1, inplace=True)\n",
    "    alleleDf.reset_index(drop=True, inplace=True)\n",
    "    alleleDf.to_csv(analysedAllelesPath, sep='\\t')\n",
    "    return alleleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:19.855Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    alleleDf_out = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:20.607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(alleleDf) == len(alleleDf_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:21.199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf_out[(alleleDf_out.unphased == True) & (alleleDf_out.PO_single == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:21.758Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf_out[(alleleDf_out.unphased == False) & (alleleDf_out.PO_single == True)]['Query'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:22.521Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf_out.allele_source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:23.539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phFullAlleleDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:24.255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alleleDf_out[(alleleDf_out.allele_source == 'PO') & (alleleDf_out.PO_single == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-27T06:06:24.880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(alleleDf_out.PO_single == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:49:47.072255Z",
     "start_time": "2018-02-20T02:49:46.220172Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## FIGURE PLOTTING ##########\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%\\n({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "def autolabel(rects, labels, ax, fontsize):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for i, rect in enumerate(rects):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height, str(labels[i]), ha='center', va='bottom', fontsize=fontsize)\n",
    "\n",
    "def getNumNoAlleles(pProteinFastaFile, alleleDf):\n",
    "    with open(pProteinFastaFile) as pProteinFasta:\n",
    "        pProteinList = []\n",
    "        for line in pProteinFasta:\n",
    "            if line.startswith('>'):\n",
    "                pProteinList.append(line[1:].strip())\n",
    "\n",
    "    assert(len(pProteinList) == len(set(pProteinList)))\n",
    "\n",
    "\n",
    "    pairedPProteinList = list(alleleDf['Query'])\n",
    "    pairedPProteinList += list(alleleDf['Target'])\n",
    "    pairedPProteinList = set(pairedPProteinList)\n",
    "    \n",
    "    for pairedPProtein in pairedPProteinList:\n",
    "        if pairedPProtein in pProteinList:\n",
    "            pProteinList.remove(pairedPProtein)\n",
    "    \n",
    "    return len(pProteinList)\n",
    "\n",
    "def plotAlleleTypesPie(ax, alleleDf, colors, includeNoAlleles=True):\n",
    "    '''Plots a pie chart of allele types, with the option of also including \n",
    "    primary proteins with no alleles. Strictly, this is not an accurate representation\n",
    "    of the distribution of primary proteins as the reciprocal BLAST-identified (h on p) alleles\n",
    "    may result in double-counting of primary proteins.\n",
    "    '''\n",
    "    # OrderedDict to preserve order, so that plots are coloured with same key as the distance \n",
    "    # bar graphs. This is a bit of a hack-fix; must enter these by hand again in the same order \n",
    "    # as 'matchType' occurs in the alleleAveragesByMatchType DataFrame.\n",
    "    alleleTypeCountDict = collections.OrderedDict()\n",
    "    \n",
    "    for matchType in alleleDf['matchType'].unique():\n",
    "        alleleTypeCountDict[matchType] = len(alleleDf[alleleDf['matchType'] == matchType])\n",
    "    \n",
    "    if includeNoAlleles==True:\n",
    "        numNoAlleles = getNumNoAlleles(P_PROTEINS_FASTA, alleleDf)\n",
    "        alleleTypeCountDict['no_allele'] = numNoAlleles\n",
    "\n",
    "    patches, texts, autotexts = ax.pie(list(alleleTypeCountDict.values()), labels=alleleTypeCountDict.keys(), autopct=make_autopct(list(alleleTypeCountDict.values())), colors=colors)\n",
    "    ax.axis('equal')\n",
    "    ax.set_title('Allele Types', loc='center', fontsize=TITLE_SIZE, position=(0.5, 1.1))\n",
    "\n",
    "def plotLevenshteinBar(alleleAverages, ax, colors):\n",
    "    '''Plots a bar graph of normalised Levenshtein distances on ax from DataFrame alleleAverages.'''\n",
    "    \n",
    "    ind = np.arange(len(alleleAverages.protein_levenshtein))\n",
    "    rects = ax.bar(ind, alleleAverages.protein_levenshtein, 0.35, color=colors, align='center') \n",
    "    \n",
    "    sns.despine(top=True, right=True)\n",
    "    \n",
    "    barLabels = []\n",
    "    for levDist in alleleAverages.protein_levenshtein:\n",
    "        barLabels.append(str(int((1-levDist)*100)) + '%')\n",
    "    autolabel(rects, barLabels, ax, INLINE_LABEL_SIZE)\n",
    "\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(alleleAverages.index, rotation=45)\n",
    "\n",
    "    # ax.set_xlabel('Allele Types', fontsize=AXIS_LABEL_SIZE)\n",
    "    ax.set_ylabel('Normalised Levenshtein Distance', fontsize=AXIS_LABEL_SIZE)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=AXIS_TICK_SIZE, pad=3)\n",
    "\n",
    "    for tick in ax.get_xaxis().get_major_ticks():\n",
    "        tick.set_pad(2*tick.get_pad())\n",
    "        tick.label1 = tick._get_text1()\n",
    "        \n",
    "def plotAlleles(alleleDf, qCovFilters, tCovFilters, pctIdFilters, levSimFilters, leavePO):\n",
    "    '''Makes a 3x2 plot with normalised Levenshtein distance plots in column 1 and\n",
    "    a pie chart representing the distribution of allele types in column 2.\n",
    "    Each row shows different levels of filtering.\n",
    "    \n",
    "    leavePO is a boolean that determines whether only BLAST hits will be filtered (leavePO=True)\n",
    "    or both BLAST and PO alleles should be filtered (leavePO=False)'''\n",
    "    cmap = plt.cm.Greens\n",
    "    colors = cmap(np.linspace(0.0, 0.6, len(alleleDf['matchType'].unique())))\n",
    "    \n",
    "    assert(len(qCovFilters) == len(pctIdFilters) == len(levSimFilters))\n",
    "    \n",
    "    fig, ax = plt.subplots(len(qCovFilters), 2, figsize=(30, 12*len(qCovFilters)))\n",
    "    \n",
    "    for i in range(len(qCovFilters)):\n",
    "        \n",
    "        filteredAlleleDf = filterAlleleDf(alleleDf, qCovFilters[i], tCovFilters[i], pctIdFilters[i], levSimFilters[i], True)\n",
    "        print(filteredAlleleDf[filteredAlleleDf.allele_source == 'PO'].shape[0])\n",
    "        # levenshtein distance plot\n",
    "        alleleAveragesByMatchType = filteredAlleleDf.groupby(['matchType']).mean()\n",
    "        plotLevenshteinBar(alleleAveragesByMatchType, ax[i, 0], colors)\n",
    "        ax[i, 0].set_xticklabels(alleleAveragesByMatchType.index, rotation=45, ha='right')\n",
    "        \n",
    "        # pie plot\n",
    "        plotAlleleTypesPie(ax[i, 1], filteredAlleleDf, colors)\n",
    "        \n",
    "        # include filtering criteria in title\n",
    "        qCovFilter = qCovFilters[i]\n",
    "        tCovFilter = tCovFilters[i]\n",
    "        pctIdFilter = pctIdFilters[i]\n",
    "        levSimFilter = levSimFilters[i]\n",
    "        if qCovFilter < BASE_QCOV_CUTOFF:\n",
    "            qCovFilter = BASE_QCOV_CUTOFF\n",
    "            print('Base QCov cut-off is 70%; if you desire to filter below this value, decrease BASE_QCOV_CUTOFF.')\n",
    "        if tCovFilter < BASE_TCOV_CUTOFF:\n",
    "            tCovFilter = BASE_TCOV_CUTOFF\n",
    "            print('Base TCov cut-off is 70%; if you desire to filter below this value, decrease BASE_TCOV_CUTOFF.')\n",
    "        if pctIdFilter < BASE_PCTID_CUTOFF:\n",
    "            pctIdFilter = BASE_PCTID_CUTOFF\n",
    "            print('Base %ID cut-off is 70%; if you desire to filter below this value, decrease BASE_PCTID_CUTOFF.')\n",
    "        if not levSimFilter:\n",
    "            levSimFilter = 0\n",
    "    \n",
    "        ax[i, 0].set_title('QCov > %s%%, TCov > %s%%, ID > %s%%, L. sim. > %s%%, PO Filtered: %s' % (qCovFilter, tCovFilter, pctIdFilter, levSimFilter, not leavePO), position=(0.5, 0.85))\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(FIGURE_PATH, 'fig'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T02:50:17.346988Z",
     "start_time": "2018-02-20T02:49:49.617825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used in the pie chart for all text except title\n",
    "# the ax.pie plotting interface is weird - cannot set other font sizes properly?\n",
    "mpl.rcParams['font.size'] = 24\n",
    "\n",
    "TITLE_SIZE = 32\n",
    "AXIS_LABEL_SIZE = 28\n",
    "AXIS_TICK_SIZE = 24\n",
    "INLINE_LABEL_SIZE = 24\n",
    "\n",
    "# These lists must all be the same length\n",
    "qCovFilters = [False, 80, 90, 95]\n",
    "pctIdFilters = [False, 80, 90, 95]\n",
    "tCovFilters = [False, 80, 90, 95]\n",
    "levSimFilters = [False, False, False, False]\n",
    "\n",
    "plotAlleles(alleleDf, qCovFilters, tCovFilters, pctIdFilters, levSimFilters, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
